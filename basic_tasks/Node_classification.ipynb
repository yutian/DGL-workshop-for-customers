{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Learning Tasks\n",
    "In this tutorial, we are going to demonstrate some basic tasks in graph learning. In general, many of the graph learning problems can fall into the following categories:\n",
    "\n",
    "* Node classification: assign a label to a node.\n",
    "* Link prediction: predict the existence of an edge between two nodes.\n",
    "* Graph classification: assign a label to a graph.\n",
    "\n",
    "Many real-world applications can be formulated as one of these graph problems.\n",
    "* Fraud detection in financial transactions: transactions form a graph, where users are nodes and transactions are edges. In this case, we want to detect malicious users, which is to assign binary labels to users.\n",
    "* Community detection in a social network: a social network is naturally a graph, where nodes are users and edges are interactions between users. We want to predict which community a node belongs to.\n",
    "* Recommendation: users and items form a bipartite graph. They are connected with edges when users purchase items. Given users' purchase history, we want to predict what items a user will purchase in a near future. Thus, recommendation is a link prediction problem.\n",
    "* Drug discovery: a molecule is a graph whose nodes are atoms. We want to predict the property of a molecule. In this case, we want to assign a label to a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get started\n",
    "\n",
    "DGL can be used with different deep learning frameworks. Currently, DGL can be used with Pytorch and MXNet. Here, we show how DGL works with Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we load DGL, we need to set the DGL backend for one of the deep learning frameworks. Because this tutorial develops models in Pytorch, we have to set the DGL backend to Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl import DGLGraph\n",
    "\n",
    "# Load Pytorch as backend\n",
    "dgl.load_backend('pytorch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the rest of the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN model\n",
    "\n",
    "Typically, GNN is used to compute meaningful node embeddings. With the embeddings, we can perform many downstream tasks.\n",
    "\n",
    "DGL provides two ways of implementing a GNN model:\n",
    "* using the [nn module](https://doc.dgl.ai/features/nn.html), which contains many commonly used GNN modules.\n",
    "* using the message passing interface to implement a GNN model from scratch.\n",
    "\n",
    "For simplicity, we implement the GNN model in the tutorial with the nn module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we use [GraphSage](https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf), one of the first inductive GNN models. GraphSage performs the following computation on every node $v$ in the graph:\n",
    "\n",
    "$$h_{N(v)}^{(l)} \\gets AGGREGATE_k({h_u^{(l-1)}, \\forall u \\in N(v)})$$\n",
    "$$h_v^{(l)} \\gets \\sigma(W^k \\cdot CONCAT(h_v^{(l-1)}, h_{N(v)}^{(l)})),$$\n",
    "\n",
    "where $N(v)$ is the neighborhood of node $v$ and $l$ is the layer Id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GraphSage model has multiple layers. In each layer, a vertex accesses its direct neighbors. When we stack $k$ layers in a model, a node $v$ access neighbors within $k$ hops. The output of the GraphSage model is node embeddings that represent the nodes and all information in the k-hop neighborhood.\n",
    "\n",
    "<img src=\"https://github.com/zheng-da/DGL_devday_tutorial/raw/master/GNN.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use DGL's `nn` module to build the GraphSage model. `SAGEConv` implements the operations of `GraphSage` in a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from dgl.nn.pytorch import conv as dgl_conv\n",
    "\n",
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 out_dim,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout,\n",
    "                 aggregator_type):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # input layer\n",
    "        self.layers.append(dgl_conv.SAGEConv(in_feats, n_hidden, aggregator_type,\n",
    "                                         feat_drop=dropout, activation=activation))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(dgl_conv.SAGEConv(n_hidden, n_hidden, aggregator_type,\n",
    "                                             feat_drop=dropout, activation=activation))\n",
    "        # output layer\n",
    "        self.layers.append(dgl_conv.SAGEConv(n_hidden, out_dim, aggregator_type,\n",
    "                                         feat_drop=dropout, activation=None))\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interested readers can check out our [online tutorials](https://doc.dgl.ai/tutorials/models/index.html) to see how to use DGL's message passing interface to implement GNN models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset for the tutorial\n",
    "\n",
    "DGL has a large collection of built-in datasets. Please see [this doc](https://doc.dgl.ai/api/python/data.html) for more information.\n",
    "\n",
    "In this tutorial, we use a citation network called pubmed for demonstration. A node in the citation network is a paper and an edge represents the citation between two papers. This dataset has 19,717 papers and 88,651 citations. Each paper has a sparse bag-of-words feature vector and a class label.\n",
    "\n",
    "All other graph data, such as node features, are stored as NumPy tensors. When we load the tensors, we convert them to Pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished data loading and preprocessing.\n",
      "  NumNodes: 19717\n",
      "  NumEdges: 88651\n",
      "  NumFeats: 500\n",
      "  NumClasses: 3\n",
      "  NumTrainingSamples: 60\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n"
     ]
    }
   ],
   "source": [
    "from dgl.data import citegrh\n",
    "\n",
    "# load and preprocess the pubmed dataset\n",
    "data = citegrh.load_pubmed()\n",
    "\n",
    "# sparse bag-of-words features of papers\n",
    "features = torch.FloatTensor(data.features)\n",
    "# the number of input node features\n",
    "in_feats = features.shape[1]\n",
    "# class labels of papers\n",
    "labels = torch.LongTensor(data.labels)\n",
    "# the number of unique classes on the nodes.\n",
    "n_classes = data.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(19717,)\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0031, 0.0000,\n",
      "        0.0000, 0.0000, 0.0302, 0.0000, 0.0082, 0.0108, 0.0000, 0.0110, 0.0000,\n",
      "        0.0064, 0.0164, 0.0000, 0.0000, 0.0080, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0131, 0.0000, 0.0000, 0.0097, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0136, 0.0107, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0122, 0.0000, 0.0179, 0.0000, 0.0218,\n",
      "        0.0000, 0.0000, 0.0000, 0.0099, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0387, 0.0000, 0.0144,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0098,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0300, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0100, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0306, 0.0089, 0.0133, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0457, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0156, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0088, 0.0000, 0.0289, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0287, 0.0295, 0.0000, 0.0000, 0.0000, 0.0000, 0.0456, 0.0000,\n",
      "        0.0341, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0225, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0137, 0.0000, 0.0000,\n",
      "        0.0164, 0.0000, 0.0073, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0152, 0.0000, 0.0000, 0.0000, 0.0000, 0.0097, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0228, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0099, 0.0000, 0.0000,\n",
      "        0.0169, 0.0000, 0.0117, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0077, 0.0150, 0.0000, 0.0000, 0.0000, 0.0458, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0084,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0130, 0.0000, 0.0000,\n",
      "        0.0000, 0.0147, 0.0320, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0134, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0128, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0139, 0.0000, 0.0125, 0.0120, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0125, 0.0000, 0.0162, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0283, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "print(n_classes)\n",
    "print(data.labels.shape)\n",
    "print(features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For small datasets, DGL stores the network structure in a [NetworkX](https://networkx.github.io) object. NetworkX is a very popular Python graph library. It provides comprehensive API for graph manipulation and is very useful for preprocessing small graphs.\n",
    "\n",
    "Then we create a DGLGraph from the grpah dataset and convert it to a read-only DGLGraph, which supports more efficient computation. Currently, DGL sampling API only works on read-only DGLGraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "g = DGLGraph(data.graph)\n",
    "g.readonly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node classification in the semi-supervised setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us perform node classification in a semi-supervised setting. In this setting, we have the entire graph structure and all node features. We only have labels on some of the nodes. We want to predict the labels on other nodes. Even though some of the nodes do not have labels, they connect with nodes with labels. Thus, we train the model with both labeled nodes and unlabeled nodes. Semi-supervised learning can usually improve performance.\n",
    "\n",
    "<img src=\"https://github.com/zheng-da/DGL_devday_tutorial/raw/master/node_classify1.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "This dependency graph shows a better view of how labeled and unlabled nodes are used in the training.\n",
    "<img src=\"https://github.com/zheng-da/DGL_devday_tutorial/raw/master/node_classify2.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a 2-layer GraphSage model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_hidden = 64\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "aggregator_type = 'gcn'\n",
    "\n",
    "gconv_model = GraphSAGEModel(in_feats,\n",
    "                             n_hidden,\n",
    "                             n_classes,\n",
    "                             n_layers,\n",
    "                             F.relu,\n",
    "                             dropout,\n",
    "                             aggregator_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the node classification model based on the GraphSage model. The GraphSage model takes a DGLGraph object and node features as input and computes node embeddings as output. With node embeddings, we use a cross entropy loss to train the node classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the node classification model based on the GraphSage model. The GraphSage model takes a DGLGraph object and node features as input and computes node embeddings as output. With node embeddings, we use a cross entropy loss to train the node classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class NodeClassification(nn.Module):\n",
    "    def __init__(self, gconv_model, n_hidden, n_classes):\n",
    "        super(NodeClassification, self).__init__()\n",
    "        self.gconv_model = gconv_model\n",
    "        self.loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, g, features, train_idx):\n",
    "        logits = self.gconv_model(g, features)\n",
    "        return self.loss_fcn(logits[train_idx], labels[train_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining a model for node classification, we need to define an evaluation function to evaluate the performance of a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def NCEvaluate(model, g, features, labels, test_idx):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # compute embeddings with GNN\n",
    "        logits = model.gconv_model(g, features)\n",
    "        logits = logits[test_idx]\n",
    "        test_labels = labels[test_idx]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == test_labels)\n",
    "        return correct.item() * 1.0 / len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for semi-supervised node classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Data statistics------'\n",
      "      #Classes 3\n",
      "      #Train samples 60\n",
      "      #Val samples 500\n",
      "      #Test samples 1000\n"
     ]
    }
   ],
   "source": [
    "# the dataset is split into training set, validation set and testing set.\n",
    "train_idx = np.where(data.train_mask > 0)[0]\n",
    "val_idx = np.where(data.val_mask > 0)[0]\n",
    "test_idx = np.where(data.test_mask > 0)[0]\n",
    "\n",
    "print(\"\"\"----Data statistics------'\n",
    "      #Classes %d\n",
    "      #Train samples %d\n",
    "      #Val samples %d\n",
    "      #Test samples %d\"\"\" %\n",
    "          (n_classes,\n",
    "           train_idx.shape[0],\n",
    "           val_idx.shape[0],\n",
    "           test_idx.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the model and evaluation function, we can put everything into the training loop to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 1.1155 | Accuracy 0.1960\n",
      "Epoch 00001 | Loss 1.0978 | Accuracy 0.1960\n",
      "Epoch 00002 | Loss 1.0900 | Accuracy 0.1960\n",
      "Epoch 00003 | Loss 1.0897 | Accuracy 0.1960\n",
      "Epoch 00004 | Loss 1.0964 | Accuracy 0.1960\n",
      "Epoch 00005 | Loss 1.0907 | Accuracy 0.2240\n",
      "Epoch 00006 | Loss 1.0923 | Accuracy 0.3820\n",
      "Epoch 00007 | Loss 1.0728 | Accuracy 0.4540\n",
      "Epoch 00008 | Loss 1.0824 | Accuracy 0.5060\n",
      "Epoch 00009 | Loss 1.0843 | Accuracy 0.5260\n",
      "Epoch 00010 | Loss 1.0918 | Accuracy 0.5400\n",
      "Epoch 00011 | Loss 1.0901 | Accuracy 0.5560\n",
      "Epoch 00012 | Loss 1.0798 | Accuracy 0.5660\n",
      "Epoch 00013 | Loss 1.0862 | Accuracy 0.6180\n",
      "Epoch 00014 | Loss 1.0860 | Accuracy 0.6480\n",
      "Epoch 00015 | Loss 1.0757 | Accuracy 0.6600\n",
      "Epoch 00016 | Loss 1.0601 | Accuracy 0.6700\n",
      "Epoch 00017 | Loss 1.0654 | Accuracy 0.6800\n",
      "Epoch 00018 | Loss 1.0639 | Accuracy 0.6880\n",
      "Epoch 00019 | Loss 1.0658 | Accuracy 0.6940\n",
      "Epoch 00020 | Loss 1.0638 | Accuracy 0.6980\n",
      "Epoch 00021 | Loss 1.0605 | Accuracy 0.6980\n",
      "Epoch 00022 | Loss 1.0426 | Accuracy 0.7080\n",
      "Epoch 00023 | Loss 1.0647 | Accuracy 0.7140\n",
      "Epoch 00024 | Loss 1.0458 | Accuracy 0.7120\n",
      "Epoch 00025 | Loss 1.0472 | Accuracy 0.7120\n",
      "Epoch 00026 | Loss 1.0506 | Accuracy 0.7160\n",
      "Epoch 00027 | Loss 1.0295 | Accuracy 0.7160\n",
      "Epoch 00028 | Loss 1.0434 | Accuracy 0.7160\n",
      "Epoch 00029 | Loss 1.0424 | Accuracy 0.7200\n",
      "Epoch 00030 | Loss 1.0206 | Accuracy 0.7200\n",
      "Epoch 00031 | Loss 1.0309 | Accuracy 0.7200\n",
      "Epoch 00032 | Loss 1.0368 | Accuracy 0.7220\n",
      "Epoch 00033 | Loss 1.0217 | Accuracy 0.7180\n",
      "Epoch 00034 | Loss 1.0132 | Accuracy 0.7240\n",
      "Epoch 00035 | Loss 1.0089 | Accuracy 0.7260\n",
      "Epoch 00036 | Loss 1.0088 | Accuracy 0.7240\n",
      "Epoch 00037 | Loss 1.0095 | Accuracy 0.7220\n",
      "Epoch 00038 | Loss 1.0017 | Accuracy 0.7200\n",
      "Epoch 00039 | Loss 0.9923 | Accuracy 0.7200\n",
      "Epoch 00040 | Loss 0.9831 | Accuracy 0.7220\n",
      "Epoch 00041 | Loss 0.9868 | Accuracy 0.7220\n",
      "Epoch 00042 | Loss 0.9671 | Accuracy 0.7220\n",
      "Epoch 00043 | Loss 0.9730 | Accuracy 0.7220\n",
      "Epoch 00044 | Loss 0.9616 | Accuracy 0.7280\n",
      "Epoch 00045 | Loss 0.9512 | Accuracy 0.7280\n",
      "Epoch 00046 | Loss 0.9521 | Accuracy 0.7240\n",
      "Epoch 00047 | Loss 0.9533 | Accuracy 0.7220\n",
      "Epoch 00048 | Loss 0.9413 | Accuracy 0.7220\n",
      "Epoch 00049 | Loss 0.9364 | Accuracy 0.7200\n",
      "Epoch 00050 | Loss 0.9423 | Accuracy 0.7240\n",
      "Epoch 00051 | Loss 0.9265 | Accuracy 0.7300\n",
      "Epoch 00052 | Loss 0.9042 | Accuracy 0.7300\n",
      "Epoch 00053 | Loss 0.8816 | Accuracy 0.7320\n",
      "Epoch 00054 | Loss 0.9041 | Accuracy 0.7320\n",
      "Epoch 00055 | Loss 0.8874 | Accuracy 0.7320\n",
      "Epoch 00056 | Loss 0.8781 | Accuracy 0.7300\n",
      "Epoch 00057 | Loss 0.8721 | Accuracy 0.7320\n",
      "Epoch 00058 | Loss 0.8669 | Accuracy 0.7280\n",
      "Epoch 00059 | Loss 0.8566 | Accuracy 0.7280\n",
      "Epoch 00060 | Loss 0.8759 | Accuracy 0.7280\n",
      "Epoch 00061 | Loss 0.8646 | Accuracy 0.7280\n",
      "Epoch 00062 | Loss 0.8325 | Accuracy 0.7320\n",
      "Epoch 00063 | Loss 0.8170 | Accuracy 0.7340\n",
      "Epoch 00064 | Loss 0.8154 | Accuracy 0.7360\n",
      "Epoch 00065 | Loss 0.8216 | Accuracy 0.7360\n",
      "Epoch 00066 | Loss 0.8043 | Accuracy 0.7360\n",
      "Epoch 00067 | Loss 0.7693 | Accuracy 0.7360\n",
      "Epoch 00068 | Loss 0.7812 | Accuracy 0.7380\n",
      "Epoch 00069 | Loss 0.7719 | Accuracy 0.7420\n",
      "Epoch 00070 | Loss 0.7459 | Accuracy 0.7440\n",
      "Epoch 00071 | Loss 0.7349 | Accuracy 0.7420\n",
      "Epoch 00072 | Loss 0.7301 | Accuracy 0.7420\n",
      "Epoch 00073 | Loss 0.7250 | Accuracy 0.7420\n",
      "Epoch 00074 | Loss 0.7303 | Accuracy 0.7420\n",
      "Epoch 00075 | Loss 0.7209 | Accuracy 0.7460\n",
      "Epoch 00076 | Loss 0.7129 | Accuracy 0.7460\n",
      "Epoch 00077 | Loss 0.6907 | Accuracy 0.7460\n",
      "Epoch 00078 | Loss 0.6781 | Accuracy 0.7480\n",
      "Epoch 00079 | Loss 0.6501 | Accuracy 0.7480\n",
      "Epoch 00080 | Loss 0.6481 | Accuracy 0.7480\n",
      "Epoch 00081 | Loss 0.6212 | Accuracy 0.7480\n",
      "Epoch 00082 | Loss 0.6423 | Accuracy 0.7480\n",
      "Epoch 00083 | Loss 0.6198 | Accuracy 0.7500\n",
      "Epoch 00084 | Loss 0.5855 | Accuracy 0.7520\n",
      "Epoch 00085 | Loss 0.5989 | Accuracy 0.7520\n",
      "Epoch 00086 | Loss 0.6026 | Accuracy 0.7480\n",
      "Epoch 00087 | Loss 0.5715 | Accuracy 0.7500\n",
      "Epoch 00088 | Loss 0.5927 | Accuracy 0.7520\n",
      "Epoch 00089 | Loss 0.5581 | Accuracy 0.7540\n",
      "Epoch 00090 | Loss 0.5173 | Accuracy 0.7600\n",
      "Epoch 00091 | Loss 0.5417 | Accuracy 0.7620\n",
      "Epoch 00092 | Loss 0.5217 | Accuracy 0.7640\n",
      "Epoch 00093 | Loss 0.4787 | Accuracy 0.7640\n",
      "Epoch 00094 | Loss 0.5607 | Accuracy 0.7640\n",
      "Epoch 00095 | Loss 0.4988 | Accuracy 0.7620\n",
      "Epoch 00096 | Loss 0.4877 | Accuracy 0.7640\n",
      "Epoch 00097 | Loss 0.4505 | Accuracy 0.7660\n",
      "Epoch 00098 | Loss 0.4841 | Accuracy 0.7660\n",
      "Epoch 00099 | Loss 0.4837 | Accuracy 0.7660\n",
      "Epoch 00100 | Loss 0.4511 | Accuracy 0.7660\n",
      "Epoch 00101 | Loss 0.4522 | Accuracy 0.7680\n",
      "Epoch 00102 | Loss 0.4314 | Accuracy 0.7680\n",
      "Epoch 00103 | Loss 0.4216 | Accuracy 0.7680\n",
      "Epoch 00104 | Loss 0.4162 | Accuracy 0.7700\n",
      "Epoch 00105 | Loss 0.4333 | Accuracy 0.7700\n",
      "Epoch 00106 | Loss 0.3948 | Accuracy 0.7740\n",
      "Epoch 00107 | Loss 0.4048 | Accuracy 0.7780\n",
      "Epoch 00108 | Loss 0.3647 | Accuracy 0.7760\n",
      "Epoch 00109 | Loss 0.3811 | Accuracy 0.7780\n",
      "Epoch 00110 | Loss 0.3784 | Accuracy 0.7800\n",
      "Epoch 00111 | Loss 0.3962 | Accuracy 0.7780\n",
      "Epoch 00112 | Loss 0.3793 | Accuracy 0.7780\n",
      "Epoch 00113 | Loss 0.3311 | Accuracy 0.7760\n",
      "Epoch 00114 | Loss 0.3416 | Accuracy 0.7720\n",
      "Epoch 00115 | Loss 0.3389 | Accuracy 0.7740\n",
      "Epoch 00116 | Loss 0.3402 | Accuracy 0.7760\n",
      "Epoch 00117 | Loss 0.3331 | Accuracy 0.7760\n",
      "Epoch 00118 | Loss 0.3252 | Accuracy 0.7780\n",
      "Epoch 00119 | Loss 0.3358 | Accuracy 0.7760\n",
      "Epoch 00120 | Loss 0.3077 | Accuracy 0.7780\n",
      "Epoch 00121 | Loss 0.2976 | Accuracy 0.7800\n",
      "Epoch 00122 | Loss 0.3028 | Accuracy 0.7800\n",
      "Epoch 00123 | Loss 0.2977 | Accuracy 0.7820\n",
      "Epoch 00124 | Loss 0.2936 | Accuracy 0.7820\n",
      "Epoch 00125 | Loss 0.2824 | Accuracy 0.7820\n",
      "Epoch 00126 | Loss 0.2943 | Accuracy 0.7820\n",
      "Epoch 00127 | Loss 0.3036 | Accuracy 0.7780\n",
      "Epoch 00128 | Loss 0.2565 | Accuracy 0.7780\n",
      "Epoch 00129 | Loss 0.2853 | Accuracy 0.7780\n",
      "Epoch 00130 | Loss 0.2710 | Accuracy 0.7780\n",
      "Epoch 00131 | Loss 0.2449 | Accuracy 0.7780\n",
      "Epoch 00132 | Loss 0.2381 | Accuracy 0.7800\n",
      "Epoch 00133 | Loss 0.2559 | Accuracy 0.7820\n",
      "Epoch 00134 | Loss 0.2155 | Accuracy 0.7820\n",
      "Epoch 00135 | Loss 0.2513 | Accuracy 0.7840\n",
      "Epoch 00136 | Loss 0.2351 | Accuracy 0.7840\n",
      "Epoch 00137 | Loss 0.2440 | Accuracy 0.7820\n",
      "Epoch 00138 | Loss 0.2214 | Accuracy 0.7820\n",
      "Epoch 00139 | Loss 0.1899 | Accuracy 0.7820\n",
      "Epoch 00140 | Loss 0.2062 | Accuracy 0.7840\n",
      "Epoch 00141 | Loss 0.2309 | Accuracy 0.7840\n",
      "Epoch 00142 | Loss 0.2099 | Accuracy 0.7820\n",
      "Epoch 00143 | Loss 0.2173 | Accuracy 0.7820\n",
      "Epoch 00144 | Loss 0.2118 | Accuracy 0.7800\n",
      "Epoch 00145 | Loss 0.2011 | Accuracy 0.7800\n",
      "Epoch 00146 | Loss 0.2036 | Accuracy 0.7800\n",
      "Epoch 00147 | Loss 0.1930 | Accuracy 0.7800\n",
      "Epoch 00148 | Loss 0.2008 | Accuracy 0.7800\n",
      "Epoch 00149 | Loss 0.1812 | Accuracy 0.7800\n",
      "\n",
      "Test Accuracy 0.7720\n"
     ]
    }
   ],
   "source": [
    "# Node classification task\n",
    "model = NodeClassification(gconv_model, n_hidden, n_classes)\n",
    "\n",
    "# Training hyperparameters\n",
    "weight_decay = 5e-4\n",
    "n_epochs = 150\n",
    "lr = 1e-3\n",
    "\n",
    "# create the Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "dur = []\n",
    "for epoch in range(n_epochs):\n",
    "    # Set the model in the training mode.\n",
    "    model.train()\n",
    "    # forward\n",
    "    loss = model(g, features, train_idx)\n",
    "    \n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # validation\n",
    "    acc = NCEvaluate(model, g, features, labels, val_idx)\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Accuracy {:.4f}\"\n",
    "          .format(epoch, loss.item(), acc))\n",
    "\n",
    "print()\n",
    "acc = NCEvaluate(model, g, features, labels, test_idx)\n",
    "print(\"Test Accuracy {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take home exercise\n",
    "\n",
    "An interested user can try other GNN models to compute node embeddings and use it for node classification. Please check out the [nn module](https://doc.dgl.ai/features/nn.html) in DGL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (DGL_Internal_Workshop_April2020)",
   "language": "python",
   "name": "pycharm-6c9a77b7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
