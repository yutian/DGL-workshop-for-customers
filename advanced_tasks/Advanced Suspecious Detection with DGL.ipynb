{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "# 首先导入需要的包\n",
    "import os\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl.nn.pytorch.softmax import edge_softmax\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGL反欺诈项目代码实践\n",
    "\n",
    "在这一任务里，我们会通过一个GNN模型的构建、探究和实验过程来学习如何使用DGL构建可被用于生产环境的代码。\n",
    "\n",
    "在本教程里，我们会完成如下的任务：\n",
    "\n",
    "1. 根据SOTA的算法论文，把计算公式转化成消息传递的模式；\n",
    "2. 根据消息传递的模式，实现单层GNN的模块；\n",
    "3. 叠加多层GNN模块，实现一个算法的模型；\n",
    "4. 使用一个小样例数据来探究模型的运行机制；\n",
    "5. 使用一个大图采样出的图数据，模拟模型的训练和推断。\n",
    "\n",
    "注：大图采样出的数据的内容不具有实际意义，所以训练和推断的结果没有参考，仅用于演示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法论文的思路转换\n",
    "\n",
    "阿里闲鱼团队在CIKM 2019上发了一篇进行[垃圾评论的检测算法](https://arxiv.org/abs/1908.10679)，被评为“最佳应用研究“论文。论文中利用了”用户“$\\to$“评论”$\\to$“商品”的二部图关系，通过含有Attention机制的特征传递方式，把“用户”和“商品”的特征加入“评论”的特征，帮助提升了对于“评论”的分类效果。\n",
    "<img src='./assets/XY-Test-Data.png' width=25%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法的原理\n",
    "\n",
    "- 通过Attention机制把“用户”+“评论”的特征发送给“商品”，并结合“商品”已有的特征，更新“商品”的特征；\n",
    "- 通过Attention机制把“商品”+“评论”的特征发送给“用户”，并结合“用户”已有的特征，更新“用户”的特征；\n",
    "- “用户”+“评论”+“商品”特征发给“评论”，并更新“评论”的特征；\n",
    "- 最后使用“用户”$||$“笔记”$||$“评论”作为特征进行“评论”的分类。其中，$||$是$concatenate$的意思。\n",
    "\n",
    "### 算法的核心公式\n",
    "可以看到，闲鱼的算法是针对一个二部图的双向重复计算。所以下面的公式介绍只解释一个方向的公式。\n",
    "\n",
    "- 对于一种点-“用户”的计算：\n",
    "\n",
    "    1. 首先把一个用户评论过的所有**商品**的特征和评论的特征concatenate起来。\n",
    "$$\\mathcal{H}_{IE}^{l-1} = \\{concat(h_i^{l-1}, h_e^{l-1}), \\forall e=(u, i)\\in E(u)\\}$$\n",
    "\n",
    "\n",
    "    2. 结合第1步里获取的合并特征，与“用户”的特征进行Attention的计算，获得“商品”+“评论”的带注意力组合的对“用户”的特征更新。\n",
    "$$\\mathcal{H}_{N(u)}^l = \\sigma(W_U^l * ATTN_U(W_{AU}^{l-1} * h_u^{l-1}, W_{AIE}^{l-1} * \\mathcal{H}_{IE}^{l-1}))$$\n",
    "其中，\n",
    "$$ATTN_U(W_{AU}^{l-1} * h_u^{l-1}, W_{AIE}^{l-1} * \\mathcal{H}_{IE}^{l-1}) \\implies \\mathcal{H}_{IE}^l$$\n",
    "\n",
    "\n",
    "    3. 使用“用户”自身的特征做非线性变换，然后与注意力更新后的特征进行组合，获得下一层的隐藏特征\n",
    "$$h_u^l = concat(V_U^l * h_u^{l-1}, \\mathcal{H}_{N(u)}^l)$$\n",
    "\n",
    "\n",
    "在这计算过程中，需要实现对于Attention的计算，原文中并没有给出。下面是按照[Attention is all you need](https://arxiv.org/abs/1706.03762)里面的原理给出计算公式。\n",
    "    \n",
    "    首先给出计算Attention前的符号定义：\n",
    "$$\\hat{h}_{u-1}^{l-1} = W_{AU}^{l-1} * h_u^{l-1},    \\hat{\\mathcal{H}}_{IE}^{l-1}=W_{AIE}^{l-1} * \\mathcal{H}_{IE}^{l-1}$$\n",
    "\n",
    "    a.1 通过点积计算出“商品”和“评价”对于“用户”的注意力原始值\n",
    "$$A_{attn\\_u} = \\hat{h}_{u-1}^{l-1} \\odot \\hat{\\mathcal{H}}_{IE}^{l-1}$$\n",
    "    \n",
    "    a.2 按照每个用户的“评论”边进行$softmax$计算。\n",
    "$$A_{attn\\_u} =softmax(A_{attn\\_u} )$$\n",
    "\n",
    "    a.3 把上一步softmax计算出的注意力系数和第1步合并的特征值广播相乘\n",
    "$$\\mathcal{\\hat{H}}_{IE}^l = \\mathcal{H}_{IE}^{l-1} * A_{attn\\_u}$$\n",
    "\n",
    "    a.4 最后一步把a.3的结果，按列求和，聚合成带注意力组合的对“用户”的特征更新\n",
    "$$\\mathcal{H}_{IE}^l = sum(\\mathcal{\\hat{H}}_{IE}^l,  dim=-1)$$\n",
    "    \n",
    "    \n",
    "    \n",
    "- 对边-“评论“的计算：\n",
    "$$h_e^l = \\sigma(W_E^l * concat(h_e^{l-1}, h_u^{l-1}, h_i^{l-1}))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按消息传递的模式构建模型\n",
    "\n",
    "\n",
    "\n",
    "下面首先构建GNN的一层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer(nn.Module):\n",
    "    \"\"\"\n",
    "    This layer is designed specifically for user Xianyu Graph algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_u_in, d_u_out, d_e_in, d_e_out, d_i_in, d_i_out):\n",
    "        super(layer, self).__init__()\n",
    "        self.act = F.relu\n",
    "\n",
    "        # 上面公式里对“评论”的三个权重，这里使用了先线性计算再concat的同质方式 \n",
    "        self.W_Ee = nn.Linear(d_e_in, d_e_out, bias=False)\n",
    "        self.W_Eu = nn.Linear(d_u_in, d_e_out, bias=False)\n",
    "        self.W_Ei = nn.Linear(d_i_in, d_e_out, bias=False)\n",
    "\n",
    "        # 上面公式里第2步的第二个和第三个权重，这里是双向的：u->i 和 i->u\n",
    "        # 1. i -> u 的权重\n",
    "        self.d_attn_ie_in = d_e_in + d_i_in\n",
    "        self.d_attn_u_out = self.d_attn_ie_in\n",
    "        self.W_ATTN_ie = nn.Linear(self.d_attn_ie_in, self.d_attn_u_out, bias=False)\n",
    "        self.W_ATTN_u = nn.Linear(d_u_in, self.d_attn_u_out, bias=False)\n",
    "\n",
    "        # 2. u -> i 的权重\n",
    "        self.d_attn_ue_in = d_e_in + d_u_in\n",
    "        self.d_attn_i_out = self.d_attn_u_out\n",
    "        self.W_ATTN_ue = nn.Linear(self.d_attn_ue_in, self.d_attn_i_out, bias=False)\n",
    "        self.W_ATTN_i = nn.Linear(d_i_in, self.d_attn_i_out, bias=False)\n",
    "\n",
    "        # 上面公式第2步里的第一权重，也是分u和i的\n",
    "        self.d_wu_in = self.d_attn_u_out\n",
    "        self.d_wu_out = int(d_u_out / 2)\n",
    "        self.W_nu = nn.Linear(self.d_wu_in, self.d_wu_out, bias=False)\n",
    "\n",
    "        self.d_wi_in = self.d_attn_i_out\n",
    "        self.d_wi_out = int(d_i_out / 2)\n",
    "        self.W_ni = nn.Linear(self.d_wi_in, self.d_wi_out, bias=False)\n",
    "\n",
    "        # 上面第3步里的权重\n",
    "        self.d_vu_out = d_u_out - self.d_wu_out\n",
    "        self.W_u = nn.Linear(d_u_in, self.d_vu_out, bias=False)\n",
    "\n",
    "        self.d_vi_out = d_i_out - self.d_wi_out\n",
    "        self.W_i = nn.Linear(d_i_in, self.d_vi_out, bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, graph, u_feats, e_feats, i_feats):\n",
    "        \"\"\"\n",
    "        Specificlly For this algorithm, feat_dict has 3 types of features:\n",
    "        'User': l-1 layer's user features, in dict {'u': features}\n",
    "        'Edge': l-1 layer's edge features, in dict {'e': features}\n",
    "        'Item': l-1 layer's note features, in dict {'i': features}\n",
    "\n",
    "        This version, we have one edge but two types:\n",
    "            - 'comment_on'\n",
    "            - 'commented_by'\n",
    "\n",
    "        :param graph: bi-partitie\n",
    "        \n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # L-1层的特征赋值\n",
    "        graph.nodes['user'].data['u'] = u_feats\n",
    "        graph.nodes['item'].data['i'] = i_feats\n",
    "        graph.edges['comment_on'].data['e'] = e_feats\n",
    "        graph.edges['commented_by'].data['e'] = e_feats\n",
    "\n",
    "        # 上面公式里的第1步concat计算\n",
    "        graph.apply_edges(lambda edges: {'h_ie': th.cat([edges.src['i'], edges.data['e']], dim=-1)}, etype='commented_by')\n",
    "        graph.apply_edges(lambda edges: {'h_ue': th.cat([edges.src['u'], edges.data['e']], dim=-1)}, etype='comment_on')\n",
    "\n",
    "        # 注意力的计算部分\n",
    "        graph.nodes['user'].data['h_attnu'] = self.W_ATTN_u(u_feats)\n",
    "        graph.nodes['item'].data['h_attni'] = self.W_ATTN_i(i_feats)\n",
    "        graph.edges['commented_by'].data['h_attne'] = self.W_ATTN_ie(graph.edges['commented_by'].data['h_ie'])\n",
    "        graph.edges['comment_on'].data['h_attne'] = self.W_ATTN_ue(graph.edges['comment_on'].data['h_ue'])\n",
    "\n",
    "        # a.1: 点积计算\n",
    "        graph.apply_edges(fn.e_dot_v('h_attne', 'h_attnu', 'edotv'), etype='commented_by')\n",
    "        graph.apply_edges(fn.e_dot_v('h_attne', 'h_attni', 'edotv'), etype='comment_on')\n",
    "\n",
    "        # a.2. 按边的softmax计算\n",
    "        graph.edges['commented_by'].data['sfm'] = edge_softmax(graph['commented_by'], graph.edges['commented_by'].data['edotv'])\n",
    "        graph.edges['comment_on'].data['sfm'] = edge_softmax(graph['comment_on'], graph.edges['comment_on'].data['edotv'])\n",
    "\n",
    "        # a.3. 广播softmax值到每条边\n",
    "        graph.apply_edges(lambda edges: {'attn': edges.data['h_attne'] * edges.data['sfm'].unsqueeze(dim=0).T},\n",
    "                          etype='commented_by')\n",
    "        graph.apply_edges(lambda edges: {'attn': edges.data['h_attne'] * edges.data['sfm'].unsqueeze(dim=0).T},\n",
    "                          etype='comment_on')\n",
    "\n",
    "        # a.4. 按列求和，聚合注意力加权后的值到端点\n",
    "        graph.update_all(fn.copy_e('attn', 'm'), fn.sum('m', 'agg_u'), etype='commented_by')\n",
    "        graph.update_all(fn.copy_e('attn', 'm'), fn.sum('m', 'agg_i'), etype='comment_on')\n",
    "\n",
    "        # 完成上面公式第2步\n",
    "        graph.nodes['user'].data['h_nu'] = self.act(self.W_nu(graph.nodes['user'].data['agg_u']))\n",
    "        graph.nodes['item'].data['h_ni'] = self.act(self.W_ni(graph.nodes['item'].data['agg_i']))\n",
    "\n",
    "        # 完成上面公式3的计算\n",
    "        graph.nodes['user'].data['u'] = th.cat([self.W_u(u_feats), graph.nodes['user'].data['h_nu']], dim=-1)\n",
    "        graph.nodes['item'].data['i'] = th.cat([self.W_i(i_feats), graph.nodes['item'].data['h_ni']], dim=-1)\n",
    "\n",
    "        # 上面公式里对“评论”的计算，这里是先矩阵相乘，再进行concat。\n",
    "        # 首先，完成矩阵相乘\n",
    "        graph.edges['comment_on'].data['h_e'] = self.W_Ee(e_feats)\n",
    "        graph.edges['commented_by'].data['h_e'] = self.W_Ee(e_feats)\n",
    "        graph.nodes['user'].data['h_u4e'] = self.W_Eu(u_feats)\n",
    "        graph.nodes['item'].data['h_i4e'] = self.W_Ei(i_feats)\n",
    "\n",
    "        # 然后，利用边的消息传递完成concat操作\n",
    "        graph.apply_edges(fn.u_add_e('h_u4e', 'h_e', 'h_ue'), etype='comment_on')\n",
    "        graph.apply_edges(fn.e_add_v('h_ue', 'h_i4e', 'e'), etype='comment_on')\n",
    "        graph.edges['comment_on'].data['e'] = self.act(graph.edges['comment_on'].data['e'])\n",
    "\n",
    "        graph.edges['commented_by'].data['e'] = graph.edges['comment_on'].data['e']\n",
    "\n",
    "        # 输出L层的特征\n",
    "        u_feats = graph.nodes['user'].data['u']\n",
    "        e_feats = graph.edges['comment_on'].data['e']\n",
    "        i_feats = graph.nodes['item'].data['i']\n",
    "\n",
    "        return u_feats, e_feats, i_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 堆叠多层形成一个GNN的模型\n",
    "\n",
    "这里我们按照通常GNN的设计，构建2层的算法模型。并再最后接一个全联接层，做一次2分类的logit输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algorithm_Model(nn.Module):\n",
    "\n",
    "    def __init__(self, u_in_dim, e_in_dim, i_in_dim,\n",
    "                 hidden_dim, out_dim_ratio=2, num_class=2):\n",
    "\n",
    "        super(Algorithm_Model, self).__init__()\n",
    "        \n",
    "        # 定义模型内部使用的隐藏层维度，这里按照2倍来进行扩展隐藏层的维度\n",
    "        u_out_dim = u_in_dim * out_dim_ratio\n",
    "        e_out_dim = e_in_dim * out_dim_ratio\n",
    "        i_out_dim = i_in_dim * out_dim_ratio\n",
    "        g_out_dim = u_out_dim + e_out_dim + i_out_dim\n",
    "        \n",
    "        # 定义2层的GNN\n",
    "        self.layer_1 = layer(u_in_dim, hidden_dim, e_in_dim, hidden_dim, i_in_dim, hidden_dim)\n",
    "        self.layer_2 = layer(hidden_dim, u_out_dim, hidden_dim, e_out_dim, hidden_dim, i_out_dim)\n",
    "        # 定义最后的输出层\n",
    "        self.output = nn.Linear(g_out_dim, out_features=num_class)\n",
    "        \n",
    "    def forward(self, graph, u_features, e_features, i_features):\n",
    "        \n",
    "        h_u, h_e, h_i = self.layer_1(graph, u_features, e_features, i_features)\n",
    "        h_u = F.relu(h_u)\n",
    "        h_e = F.relu(h_e)\n",
    "        h_i = F.relu(h_i)\n",
    "        h_u, h_e, h_i = self.layer_2(graph, h_u, h_e, h_i)\n",
    "\n",
    "        # 使用DGL的消息传递机制来concat“用户”，“商品”和“评论”的特征\n",
    "        # 先赋值\n",
    "        graph.nodes['user'].data['u'] = h_u\n",
    "        graph.nodes['item'].data['i'] = h_i\n",
    "        graph.edges['comment_on'].data['e'] = h_e\n",
    "\n",
    "        # 按照边来concat两个端点的特征\n",
    "        graph.apply_edges(lambda edges:\n",
    "                          {'output': th.cat([edges.src['u'], edges.data['e'], edges.dst['i']], dim=-1)}, \n",
    "                          etype='comment_on')\n",
    "\n",
    "        # 最后的分类输出层计算\n",
    "        output = graph.edges['comment_on'].data['output']\n",
    "        logits = self.output(output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用小样例数据来对模型的内部运作进行探究\n",
    "\n",
    "这里使用和原论文的例图结构一样一个极小数据，帮助了解模型的一层的内部运作机制，方便进一步理解算法和DGL的运作机制。\n",
    "<img src='./assets/XY-Test-Data.png' width=50%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_xy_graph(u, v):\n",
    "\n",
    "    graph = dgl.heterograph(\n",
    "        {('user', 'comment_on', 'item'): (u, v),\n",
    "         ('item', 'commented_by', 'user'): (v, u)}\n",
    "    )\n",
    "\n",
    "    return graph\n",
    "\n",
    "u = th.tensor([0,1,2,1,1,3,4,4])\n",
    "v = th.tensor([0,0,0,1,2,1,1,2])\n",
    "\n",
    "xy_graph = build_xy_graph(u, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 下面给这个图的节点和边赋一些随机的值\n",
    "\n",
    "- 用户节点: 6d，用户特征的值和用户自己对应的ID一致，例如，节点0是\\[0,0,0,0,0,0\\]，节点1是\\[1,1,1,1,1,1\\]，以此类推。\n",
    "- 商品节点: 3d, 商品特征的值和商品自己对应的ID一致，但是负数，例如，节点0是\\[0,0,0\\]，节点1是\\[-1,-1,-1\\]，以此类推。\n",
    "- 评论边:   7d, 对于“comment_on”和“commented_by”类型的边赋相同的值，特征的值和边对应的ID一致，但缩小1/10，例如，边0是\\[0,0,0,0,0,0,0\\]，边1是\\[0.1,0.1,0.1,0.1,0.1,0.1,0.1\\]，以此类推。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户输入特征：\n",
      " {'u': tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3., 3., 3.],\n",
      "        [4., 4., 4., 4., 4., 4.]])}\n",
      "商品输入特征：\n",
      " {'i': tensor([[-0., -0., -0.],\n",
      "        [-1., -1., -1.],\n",
      "        [-2., -2., -2.]])}\n",
      "评论的边的特征：\n",
      " {'e': tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000],\n",
      "        [0.4000, 0.4000, 0.4000, 0.4000, 0.4000, 0.4000, 0.4000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000],\n",
      "        [0.7000, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000]])}\n"
     ]
    }
   ],
   "source": [
    "user_feats = th.from_numpy(np.ones([5,6]) * np.arange(5).reshape(5,1)).float()\n",
    "xy_graph.nodes['user'].data['u'] = user_feats\n",
    "print('用户输入特征：\\n', xy_graph.nodes['user'].data)\n",
    "\n",
    "item_feats = th.from_numpy(np.ones([3,3]) * np.arange(3).reshape(3,1) * -1).float()\n",
    "xy_graph.nodes['item'].data['i'] = item_feats\n",
    "print('商品输入特征：\\n', xy_graph.nodes['item'].data)\n",
    "\n",
    "edge_feats = th.from_numpy(np.ones([8,7]) * np.arange(8).reshape(8,1) * 0.1).float()\n",
    "xy_graph.edges['comment_on'].data['e'] = edge_feats\n",
    "xy_graph.edges['commented_by'].data['e'] = edge_feats\n",
    "print('评论的边的特征：\\n', xy_graph.edges['comment_on'].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初始化一层的实例\n",
    "\n",
    "这里会初始化我们定义一层layer，按照上面layer类init方法的定义，用户特征输入维度是5，商品特征输入维度是3，评论边特征输入维度是7。相应的输出维度各自加倍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_layer = layer(d_u_in=6, d_u_out=20, d_e_in=7, d_e_out=20, d_i_in=3, d_i_out=20)\n",
    "\n",
    "user_out, edge_out, item_out = test_layer(xy_graph, user_feats, edge_feats, item_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验数据全图训练和推断\n",
    "\n",
    "为了展示如何完成对于这个模型的训练和推断，这里会使用从一个稍大的图数据。\n",
    "\n",
    "请注意：其中的标签是随机生成，仅用于演示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从本地文件读取图数据文件\n",
    "data_path = \"./example_graph\"\n",
    "src = pd.read_csv(os.path.join(data_path, 'e_src.csv'))\n",
    "dst = pd.read_csv(os.path.join(data_path, 'e_dst.csv'))\n",
    "e_feats = pd.read_csv(os.path.join(data_path, 'edge.csv'))\n",
    "u_feats = pd.read_csv(os.path.join(data_path, 'user.csv'))\n",
    "i_feats = pd.read_csv(os.path.join(data_path, 'item.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
